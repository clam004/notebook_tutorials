{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHMN8aLUt0l4qO7YSgjfqL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clam004/notebook_tutorials/blob/main/babyforce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install transformers "
      ],
      "metadata": {
        "id": "_BrcYn9CFWTY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5bcvqPAEDUW",
        "outputId": "79b486da-bbf0-42fb-abf1-57599959f06e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.__version__ 1.12.1+cu113\n",
            "torch.cuda.device_count() 0\n",
            "torch.cuda.empty_cache() None\n",
            "4.22.1\n",
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "#sys libs\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import datetime\n",
        "from datetime import date\n",
        "import calendar\n",
        "import pytz\n",
        "\n",
        "#data manupulation libs\n",
        "import numpy as np\n",
        "\n",
        "#string manupulation libs\n",
        "import re\n",
        "import string\n",
        "\n",
        "#torch libs\n",
        "import torch\n",
        "print('torch.__version__', torch.__version__)\n",
        "print('torch.cuda.device_count()', torch.cuda.device_count())\n",
        "print('torch.cuda.empty_cache()', torch.cuda.empty_cache())\n",
        "\n",
        "#huggingface transformers\n",
        "import transformers\n",
        "print(transformers.__version__)\n",
        "from transformers import set_seed\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import GPTJForCausalLM\n",
        "\n",
        "# seeds\n",
        "set_seed(42)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU acceleration\n",
        "\n",
        "To give yourself a GPU in colab, go to `Runtime`-->`Change runtime type`\n",
        "\n",
        "You can confirm this worked because if you run the above cell again `torch.cuda.device_count()` will change from 0 to the number of GPUs PyTorch now recognizes, this would be 0 to 1 in colab. "
      ],
      "metadata": {
        "id": "9Z8UNY7vEiC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_dialog_history = [\n",
        "    {'speaker':'bot','utterance':'Hello! who are you?'},\n",
        "    {'speaker':'human','utterance':'my name is baby force'},\n",
        "    {'speaker':'bot','utterance':'hi baby, or is it Mr. Force?'},\n",
        "    {'speaker':'human','utterance':'actually im a baby girl and my first name is Force'},\n",
        "    {'speaker':'bot','utterance':'thats a weird name for a baby girl isnt it?'},\n",
        "    {'speaker':'human','utterance':'its a weird name for any human'},\n",
        "]\n",
        "\n",
        "\n",
        "def get_background_prompt(\n",
        "    human_symbol = '[H]',\n",
        "    bot_symbol = '[B]',\n",
        "):\n",
        "\n",
        "    \"\"\" a background_prompt describing what the conversation is\n",
        "    (e.g. This is a conversation between [{client_name}], a person, and [{robot_name}] ...)\n",
        "    \"\"\"\n",
        "\n",
        "    utc_now = pytz.utc.localize(datetime.datetime.utcnow())\n",
        "    pst_now = utc_now.astimezone(pytz.timezone(\"America/Los_Angeles\"))\n",
        "    curr_date = date.today()\n",
        "    day_of_week = calendar.day_name[curr_date.weekday()]\n",
        "\n",
        "    local_date = pst_now.strftime(\"%m/%d/%Y\")\n",
        "    local_time = pst_now.strftime(\"%I:%M %p\")\n",
        "\n",
        "    background_prompt = \\\n",
        "    \"This is a text messaging only conversation between \"+human_symbol+\" and \"+bot_symbol+\". \"+\\\n",
        "    bot_symbol+\" is an artificial intelligence or AI. \"+\\\n",
        "    bot_symbol+\" knows it is just a computer program. \"+\\\n",
        "    bot_symbol+\" speaks in a manner that is kind, empathetic and \"+\\\n",
        "    \"is programmed to keep \"+human_symbol+\" safe. \"+\\\n",
        "    bot_symbol+\" likes to tell helpful stories. \"+\\\n",
        "    \"Today's date is \"+day_of_week+\" \"+local_date+\", the time is \"+local_time+\". \"+\\\n",
        "    \"\\n \"\n",
        "\n",
        "    return background_prompt\n",
        "\n",
        "\n",
        "def get_model_tokenizer(\n",
        "    model_load_path = 'EleutherAI/gpt-j-6B',\n",
        "    tokenizer_name = 'EleutherAI/gpt-j-6B',\n",
        "    cache_dir = None,\n",
        "    model_device = None,\n",
        "    verbose = False,\n",
        "):\n",
        "\n",
        "    ''' This is a function to clean up the model preparations, GPU/CPU loading \n",
        "    and matching tokenizer\n",
        "\n",
        "    model architecture is based on the tokenizer name\n",
        "\n",
        "    set model_device = 'cpu' to force model onto CPU despite having GPUs \n",
        "    available, or to torch.device('cuda:3') to get it to the fourth GPU, etc.\n",
        "    if no GPUs available, cpu is the default. \n",
        "    '''\n",
        "\n",
        "    NUM_GPUS = torch.cuda.device_count()\n",
        "\n",
        "    if tokenizer_name in ['distilgpt2', 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']:\n",
        "\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(\n",
        "            tokenizer_name,\n",
        "            pad_token='<|endoftext|>',\n",
        "            padding_side = 'left',\n",
        "        )\n",
        "\n",
        "        model = GPT2LMHeadModel.from_pretrained(\n",
        "            model_load_path,\n",
        "            cache_dir = cache_dir, \n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    elif tokenizer_name in ['EleutherAI/gpt-j-6B']:\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            tokenizer_name,\n",
        "            pad_token='<|endoftext|>',\n",
        "            padding_side = 'left',\n",
        "        )\n",
        "\n",
        "        model = GPTJForCausalLM.from_pretrained(\n",
        "            model_load_path,\n",
        "            cache_dir = cache_dir, \n",
        "        )\n",
        "    else:\n",
        "\n",
        "        if verbose:\n",
        "            print('no match for tokenizer found')\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    if model_device is not None:\n",
        "        model = model.to(model_device)\n",
        "    elif NUM_GPUS == 1:\n",
        "        if verbose:\n",
        "            print('model = model.cuda()')\n",
        "        model = model.cuda()\n",
        "    elif NUM_GPUS > 1 and tokenizer_name in ['distilgpt2', 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'EleutherAI/gpt-j-6B']:\n",
        "        # break up model and place model components on different GPUs\n",
        "        if verbose:\n",
        "            print('model.parallelize()')\n",
        "        model.parallelize()\n",
        "    else:\n",
        "        if verbose:\n",
        "            print('did not place model on any GPUs, model_device = \\'cpu\\'')\n",
        "\n",
        "    if verbose:\n",
        "        print('model.device', model.device)\n",
        "        print(\"num_params\", \n",
        "            sum(p.numel() for p in model.parameters() if p.requires_grad)/1e9,\n",
        "            \"B\"\n",
        "        ) \n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def end_punctuation(utter):\n",
        "    \n",
        "    if len(utter) > 0:\n",
        "      if utter[-1] not in [\"?\",\"!\",\".\"]:\n",
        "          utter+=\".\"\n",
        "        \n",
        "    return utter\n",
        "\n",
        "\n",
        "def extract_str(\n",
        "    reply, \n",
        "    prefix = None,\n",
        "    stop_strings = [\n",
        "        '<',\n",
        "        '[human]',\n",
        "        '\\n',\n",
        "        '[',\n",
        "    ],\n",
        "    verbose = True,\n",
        "):\n",
        "\n",
        "    \"\"\" this function clips the generated text\n",
        "    and extracts out the text between a\n",
        "    pre-specified prefix and suffix\n",
        "\n",
        "    the prefix could be the enture input text\n",
        "    the suffix is often the delimiter such as \n",
        "    the next line \\n token or a period . .\n",
        "    \"\"\"\n",
        "\n",
        "    if prefix is not None:\n",
        "        reply = reply[len(prefix):]\n",
        "\n",
        "    if verbose:\n",
        "        print('predicted future:')\n",
        "        print(repr(reply))\n",
        "    \n",
        "    for string in stop_strings:\n",
        "        if string in reply:\n",
        "            reply = reply[:reply.index(string)]\n",
        "    \n",
        "    return reply.strip()\n",
        "\n",
        "\n",
        "def convo_list_dic2list_str(\n",
        "  conversation_list_dic,\n",
        "  human_symbol = '[H]: ',\n",
        "  bot_symbol = '[B]: ',\n",
        "  utterance_delimiter = '\\n',\n",
        "):\n",
        "\n",
        "  \"\"\" This function takes a list of dictionaries\n",
        "  and turns them into a list of speaker_symbol + utterance strings\n",
        "\n",
        "  Args: \n",
        "      conversation_list_dic (List[Dict]): \n",
        "      ie: [{'speaker': 'bot', 'utterance': 'im waking up!'},\n",
        "           {'speaker': 'human', 'utterance': 'wakey wakey sleepyhead'}, ...]\n",
        "\n",
        "  Returns:\n",
        "      conversation_list_str (List[str]): list of speaker_symbol + utterance strings\n",
        "      ie: ['\\n[C]: Hello Fara.','\\n[A]: Hello! How are you doing today?',...]\n",
        "  \"\"\"\n",
        "\n",
        "  speaker2symbol = {\n",
        "      'bot':bot_symbol,\n",
        "      'human':human_symbol,\n",
        "  }\n",
        "\n",
        "  conversation_list_str = list()\n",
        "\n",
        "  for u in conversation_list_dic:\n",
        "\n",
        "      speaker_symbol = speaker2symbol[u['speaker']]\n",
        "      utterance = end_punctuation(u['utterance'])\n",
        "\n",
        "      conversation_list_str.append(utterance_delimiter + speaker_symbol + utterance)\n",
        "\n",
        "  # Elicit next agent utterance\n",
        "  conversation_list_str.append(utterance_delimiter + bot_symbol)\n",
        "\n",
        "  return conversation_list_str\n",
        "\n",
        "def generate_extract_replies(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    max_gen_len = 16, \n",
        "    no_repeat_ngram_size = None,\n",
        "    pad_token_id = 50256,\n",
        "    do_sample = True,\n",
        "    top_k = 100, \n",
        "    top_p = 0.99, \n",
        "    num_return_sequences = 1,\n",
        "    temperature = 0.9,\n",
        "    stop_strings = [\n",
        "        '<',\n",
        "        '[human]',\n",
        "        '\\n',\n",
        "        '[',\n",
        "    ],\n",
        "    verbose = False,\n",
        "):\n",
        "\n",
        "    ''' This function predicts the next utterance\n",
        "    in a conversation\n",
        "    '''\n",
        "\n",
        "    gen_texts = generate_text(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        prompt,\n",
        "        max_gen_len = max_gen_len, \n",
        "        no_repeat_ngram_size = no_repeat_ngram_size,\n",
        "        pad_token_id = pad_token_id,\n",
        "        do_sample = do_sample,\n",
        "        top_k = top_k, \n",
        "        top_p = top_p, \n",
        "        num_return_sequences = num_return_sequences,\n",
        "        temperature = temperature,\n",
        "        verbose = verbose,\n",
        "    )\n",
        "\n",
        "    replies = [\n",
        "        extract_str(\n",
        "            gen_text,\n",
        "            prefix = prompt,\n",
        "            stop_strings = stop_strings,\n",
        "            verbose = verbose,\n",
        "        )\n",
        "        for gen_text in gen_texts\n",
        "    ]\n",
        "\n",
        "    return replies\n",
        "\n",
        "\n",
        "def generate_text(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    max_gen_len = 16, \n",
        "    no_repeat_ngram_size = None,\n",
        "    pad_token_id = 50256,\n",
        "    do_sample = True,\n",
        "    top_k = 100, \n",
        "    top_p = 0.99, \n",
        "    num_return_sequences = 1,\n",
        "    temperature = 0.9,\n",
        "    verbose = False,\n",
        "):\n",
        "\n",
        "    ''' function for generating text from an input into \n",
        "    the app.package model\n",
        "\n",
        "    prompt (str): text to be tokenized and pushed through model\n",
        "\n",
        "    if you are doing few shot detection you should leave \n",
        "    no_repeat_ngram_size = None and max_len = 16\n",
        "    as long as the default max_len is more than the expected\n",
        "    label text\n",
        "\n",
        "    we leave it up to the label extractor to clip of the portion\n",
        "    of the generated text that you need\n",
        "    '''\n",
        "    NUM_GPUS = torch.cuda.device_count()\n",
        "\n",
        "    prompt_dic = tokenizer(prompt,return_tensors=\"pt\")\n",
        "    prompt_ids = prompt_dic.input_ids\n",
        "    prompt_mask = prompt_dic.attention_mask\n",
        "    prompt_len = prompt_ids.shape[1]\n",
        "\n",
        "    if verbose:\n",
        "        print('prompt_ids.shape', prompt_ids.shape)\n",
        "        print('prompt_mask.shape', prompt_mask.shape)\n",
        "\n",
        "    if NUM_GPUS > 0:\n",
        "        prompt_ids = prompt_ids.to(model.device)\n",
        "        prompt_mask = prompt_mask.to(model.device)\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        prompt_ids,\n",
        "        attention_mask = prompt_mask,\n",
        "        max_length = prompt_len + max_gen_len,\n",
        "        no_repeat_ngram_size = no_repeat_ngram_size,\n",
        "        pad_token_id = pad_token_id,\n",
        "        do_sample = do_sample,\n",
        "        top_k = top_k, \n",
        "        top_p = top_p, \n",
        "        num_return_sequences = num_return_sequences,\n",
        "        temperature = temperature,\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.batch_decode(output_ids)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self, model, tokenizer):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dialog_history = example_dialog_history\n",
        "\n",
        "    def initiate_conversation(self,):\n",
        "\n",
        "        initial_utterance = \"Hello! who are you?\"\n",
        "\n",
        "        self.dialog_history = [{'speaker':'bot','utterance':initial_utterance}]\n",
        "\n",
        "        return initial_utterance\n",
        "\n",
        "    def receive_respond(self, \n",
        "        input_utterance, \n",
        "        symbol_utter_separator=': ',\n",
        "        utterance_delimiter = '\\n',\n",
        "        human_symbol = '[Human]',\n",
        "        bot_symbol = '[AI]',\n",
        "        verbose = False,\n",
        "    ):\n",
        "\n",
        "        input_utterance = input_utterance.strip()\n",
        "\n",
        "        background_prompt =  get_background_prompt(\n",
        "            human_symbol = human_symbol,\n",
        "            bot_symbol = bot_symbol,\n",
        "        )\n",
        "\n",
        "        self.dialog_history.append({'speaker':'human','utterance':input_utterance})\n",
        "\n",
        "        convo_list_str = convo_list_dic2list_str(\n",
        "            self.dialog_history,\n",
        "            human_symbol = human_symbol+symbol_utter_separator,\n",
        "            bot_symbol = bot_symbol+symbol_utter_separator,\n",
        "            utterance_delimiter = utterance_delimiter,\n",
        "        )\n",
        "\n",
        "        background_dialog_prompt = (background_prompt + ''.join(convo_list_str)).strip()\n",
        "\n",
        "        if verbose:\n",
        "            print(repr(background_dialog_prompt))\n",
        "\n",
        "        replies = generate_extract_replies(\n",
        "            model = self.model,\n",
        "            tokenizer = self.tokenizer,\n",
        "            prompt = background_dialog_prompt,\n",
        "            max_gen_len = 32, \n",
        "            no_repeat_ngram_size = 3,\n",
        "            pad_token_id = self.tokenizer.eos_token_id,\n",
        "            do_sample = True,\n",
        "            top_k = 80, \n",
        "            top_p = 0.8, \n",
        "            num_return_sequences = 1,\n",
        "            temperature = 0.8,\n",
        "            stop_strings = [\n",
        "                human_symbol,\n",
        "                '\\n',\n",
        "            ],\n",
        "            verbose = False, \n",
        "        )\n",
        "\n",
        "        if verbose:\n",
        "            print(replies)\n",
        "\n",
        "        self.dialog_history.append({'speaker':'client','utterance':replies[0]})\n",
        "\n",
        "        return replies[0]\n",
        "\n",
        "\n",
        "def chat(agent):\n",
        "\n",
        "    print(agent.initiate_conversation())\n",
        "\n",
        "    while True:\n",
        "\n",
        "        statement = input(\"you> \")\n",
        "        print(\"agent> \",agent.receive_respond(statement, verbose = False))\n",
        "\n",
        "        if statement == \"quit\":\n",
        "            break"
      ],
      "metadata": {
        "id": "y_8ShOXIFFF_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = get_model_tokenizer(\n",
        "      model_load_path = 'gpt2-large',\n",
        "      tokenizer_name = 'gpt2-large',\n",
        "      cache_dir = '../modelstates/gpt2-large',\n",
        "      model_device = None,\n",
        "      verbose=True,\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "H8E-2BycLZ5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(model, tokenizer)\n",
        "\n",
        "chat(agent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "AiBzFemrMYaF",
        "outputId": "dc1abf6a-b909-49af-af73-a79de15f3647"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! who are you?\n",
            "you> my name is baby force\n",
            "agent>  I'm sorry, who are we?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-86d2fc3bc3af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-da7d7b1087c4>\u001b[0m in \u001b[0;36mchat\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         \u001b[0mstatement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"agent> \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_respond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}