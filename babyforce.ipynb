{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM5IQfxUwntEcDJAdG+tSqH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4bfcb45e8ad04146926095cad723d52f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a9737e250104f73bf5f34801d277baf",
              "IPY_MODEL_d32688dbf90b49b39799bfd673d8a155",
              "IPY_MODEL_83349b366ce8495a81834ec875d87e20"
            ],
            "layout": "IPY_MODEL_91f58d08ec8f48908fa3cc8dcc41dd59"
          }
        },
        "6a9737e250104f73bf5f34801d277baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea3698016e1847daa7328b57581eef5f",
            "placeholder": "​",
            "style": "IPY_MODEL_70c78e29fb0a43559f43c25b40ae8d13",
            "value": "Downloading: 100%"
          }
        },
        "d32688dbf90b49b39799bfd673d8a155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcf14912c80a4a1684312f961bb3c9fc",
            "max": 836,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f762a4def5dd4d78b0abbe3670e7180a",
            "value": 836
          }
        },
        "83349b366ce8495a81834ec875d87e20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81f908249fc744bf8bb64be5c8033bc8",
            "placeholder": "​",
            "style": "IPY_MODEL_d4806a5e373d445bb759abba2903a2a6",
            "value": " 836/836 [00:00&lt;00:00, 23.0kB/s]"
          }
        },
        "91f58d08ec8f48908fa3cc8dcc41dd59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea3698016e1847daa7328b57581eef5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70c78e29fb0a43559f43c25b40ae8d13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcf14912c80a4a1684312f961bb3c9fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f762a4def5dd4d78b0abbe3670e7180a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81f908249fc744bf8bb64be5c8033bc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4806a5e373d445bb759abba2903a2a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f3b38477e904eae9031d5cbe6b139bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7818b2ced5ce43f98548115e3218e27e",
              "IPY_MODEL_62014d34810943cfb1b02dbde471efed",
              "IPY_MODEL_d06e287c559d4b9fb28a36d58cd49caf"
            ],
            "layout": "IPY_MODEL_3fd7d34e64204dd5a607264baffb7fac"
          }
        },
        "7818b2ced5ce43f98548115e3218e27e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd873a1b96cb4975b54186f818150420",
            "placeholder": "​",
            "style": "IPY_MODEL_bfe89ed19fa0403d9a11d87c24fb8bd0",
            "value": "Downloading:   6%"
          }
        },
        "62014d34810943cfb1b02dbde471efed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21aff844bf7d4db69b036742dba97226",
            "max": 12106053103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_436fd5a7e72349d6a47e63d2223e22e9",
            "value": 668911616
          }
        },
        "d06e287c559d4b9fb28a36d58cd49caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f9c863520fd4b418377f07dae6f41c0",
            "placeholder": "​",
            "style": "IPY_MODEL_5f4b646c6da24eee9a356196403e74ee",
            "value": " 669M/12.1G [00:11&lt;04:06, 46.5MB/s]"
          }
        },
        "3fd7d34e64204dd5a607264baffb7fac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd873a1b96cb4975b54186f818150420": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfe89ed19fa0403d9a11d87c24fb8bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21aff844bf7d4db69b036742dba97226": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "436fd5a7e72349d6a47e63d2223e22e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f9c863520fd4b418377f07dae6f41c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f4b646c6da24eee9a356196403e74ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clam004/notebook_tutorials/blob/main/babyforce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minimal Causal Large Language Model (LLM) Chatbot\n",
        "\n",
        "run the next two cells to install and import PyTorch and huggingface "
      ],
      "metadata": {
        "id": "JGkuWFiyNpi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install transformers accelerate"
      ],
      "metadata": {
        "id": "_BrcYn9CFWTY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5bcvqPAEDUW",
        "outputId": "75f8088d-2a32-46f9-d73c-28a4de3e0246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.__version__ 1.12.1+cu113\n",
            "torch.cuda.device_count() 1\n",
            "torch.cuda.empty_cache() None\n",
            "4.22.1\n",
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "#sys libs\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import datetime\n",
        "from datetime import date\n",
        "import calendar\n",
        "import pytz\n",
        "\n",
        "#data manupulation libs\n",
        "import numpy as np\n",
        "\n",
        "#string manupulation libs\n",
        "import re\n",
        "import string\n",
        "\n",
        "#torch libs\n",
        "import torch\n",
        "print('torch.__version__', torch.__version__)\n",
        "print('torch.cuda.device_count()', torch.cuda.device_count())\n",
        "print('torch.cuda.empty_cache()', torch.cuda.empty_cache())\n",
        "\n",
        "#huggingface transformers\n",
        "import transformers\n",
        "print(transformers.__version__)\n",
        "from transformers import set_seed\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import GPTJForCausalLM\n",
        "\n",
        "# seeds\n",
        "set_seed(42)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU acceleration\n",
        "\n",
        "To give yourself a GPU in colab, go to `Runtime`-->`Change runtime type`\n",
        "\n",
        "You can confirm this worked because if you run the above cell again `torch.cuda.device_count()` will change from 0 to the number of GPUs PyTorch now recognizes, this would be 0 to 1 in colab. \n",
        "\n",
        "\n",
        "### The Agent and Environment\n",
        "\n",
        "For transparency the cell below has all the function and classes that run this minimal demo, it is very long, so run it and scroll all the way to the bottom to get to the demo. You can make code changes and quickly see the results, for example, change the get_background_prompt() function to chnage the initial part of the LLM input prompt that come before the dialog history portion of the final prompt. "
      ],
      "metadata": {
        "id": "9Z8UNY7vEiC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_dialog_history = [\n",
        "    {'speaker':'bot','utterance':'Hello! who are you?'},\n",
        "    {'speaker':'human','utterance':'my name is baby force'},\n",
        "    {'speaker':'bot','utterance':'hi baby, or is it Mr. Force?'},\n",
        "    {'speaker':'human','utterance':'actually im a baby girl and my first name is Force'},\n",
        "    {'speaker':'bot','utterance':'thats a weird name for a baby girl isnt it?'},\n",
        "    {'speaker':'human','utterance':'its a weird name for any human'},\n",
        "]\n",
        "\n",
        "\n",
        "def get_background_prompt(\n",
        "    human_symbol = '[H]',\n",
        "    bot_symbol = '[B]',\n",
        "):\n",
        "\n",
        "    \"\"\" a background_prompt describing what the conversation is\n",
        "    (e.g. This is a conversation between [{client_name}], a person, and [{robot_name}] ...)\n",
        "    \"\"\"\n",
        "\n",
        "    utc_now = pytz.utc.localize(datetime.datetime.utcnow())\n",
        "    pst_now = utc_now.astimezone(pytz.timezone(\"America/Los_Angeles\"))\n",
        "    curr_date = date.today()\n",
        "    day_of_week = calendar.day_name[curr_date.weekday()]\n",
        "\n",
        "    local_date = pst_now.strftime(\"%m/%d/%Y\")\n",
        "    local_time = pst_now.strftime(\"%I:%M %p\")\n",
        "\n",
        "    background_prompt = \\\n",
        "    \"This is a text messaging only conversation between \"+human_symbol+\" and \"+bot_symbol+\". \"+\\\n",
        "    bot_symbol+\" is an artificial intelligence or AI. \"+\\\n",
        "    bot_symbol+\" knows it is just a computer program. \"+\\\n",
        "    bot_symbol+\" speaks in a manner that is kind, empathetic and \"+\\\n",
        "    \"is programmed to keep \"+human_symbol+\" safe. \"+\\\n",
        "    bot_symbol+\" likes to tell helpful stories. \"+\\\n",
        "    \"Today's date is \"+day_of_week+\" \"+local_date+\", the time is \"+local_time+\". \"+\\\n",
        "    \"\\n \"\n",
        "\n",
        "    return background_prompt\n",
        "\n",
        "\n",
        "def get_model_tokenizer(\n",
        "    model_load_path = 'EleutherAI/gpt-j-6B',\n",
        "    tokenizer_name = 'EleutherAI/gpt-j-6B',\n",
        "    cache_dir = None,\n",
        "    model_device = None,\n",
        "    verbose = False,\n",
        "):\n",
        "\n",
        "    ''' This is a function to clean up the model preparations, GPU/CPU loading \n",
        "    and matching tokenizer\n",
        "\n",
        "    model architecture is based on the tokenizer name\n",
        "\n",
        "    set model_device = 'cpu' to force model onto CPU despite having GPUs \n",
        "    available, or to torch.device('cuda:3') to get it to the fourth GPU, etc.\n",
        "    if no GPUs available, cpu is the default. \n",
        "    '''\n",
        "\n",
        "    NUM_GPUS = torch.cuda.device_count()\n",
        "\n",
        "    if tokenizer_name in ['distilgpt2', 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']:\n",
        "\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(\n",
        "            tokenizer_name,\n",
        "            pad_token='<|endoftext|>',\n",
        "            padding_side = 'left',\n",
        "        )\n",
        "\n",
        "        model = GPT2LMHeadModel.from_pretrained(\n",
        "            model_load_path,\n",
        "            cache_dir = cache_dir, \n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    elif tokenizer_name in ['EleutherAI/gpt-j-6B']:\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            tokenizer_name,\n",
        "            pad_token='<|endoftext|>',\n",
        "            padding_side = 'left',\n",
        "        )\n",
        "\n",
        "        if NUM_GPUS > 0:\n",
        "          \n",
        "          model = GPTJForCausalLM.from_pretrained(\n",
        "              model_load_path,\n",
        "              revision='float16', \n",
        "              torch_dtype=torch.float16, \n",
        "              low_cpu_mem_usage=True,\n",
        "              cache_dir = cache_dir, \n",
        "          )\n",
        "\n",
        "        else:\n",
        "\n",
        "          model = GPTJForCausalLM.from_pretrained(\n",
        "              model_load_path,\n",
        "              cache_dir = cache_dir, \n",
        "          )\n",
        "\n",
        "    else:\n",
        "\n",
        "        if verbose:\n",
        "            print('no match for tokenizer found')\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    if model_device is not None:\n",
        "        model = model.to(model_device)\n",
        "    elif NUM_GPUS == 1:\n",
        "        if verbose:\n",
        "            print('model = model.cuda()')\n",
        "        model = model.cuda()\n",
        "    elif NUM_GPUS > 1 and tokenizer_name in ['distilgpt2', 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'EleutherAI/gpt-j-6B']:\n",
        "        # break up model and place model components on different GPUs\n",
        "        if verbose:\n",
        "            print('model.parallelize()')\n",
        "        model.parallelize()\n",
        "    else:\n",
        "        if verbose:\n",
        "            print('did not place model on any GPUs, model_device = \\'cpu\\'')\n",
        "\n",
        "    if verbose:\n",
        "        print('model.device', model.device)\n",
        "        print(\"num_params\", \n",
        "            sum(p.numel() for p in model.parameters() if p.requires_grad)/1e9,\n",
        "            \"B\"\n",
        "        ) \n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def end_punctuation(utter):\n",
        "    \n",
        "    if len(utter) > 0:\n",
        "      if utter[-1] not in [\"?\",\"!\",\".\"]:\n",
        "          utter+=\".\"\n",
        "        \n",
        "    return utter\n",
        "\n",
        "\n",
        "def extract_str(\n",
        "    reply, \n",
        "    prefix = None,\n",
        "    stop_strings = [\n",
        "        '<',\n",
        "        '[human]',\n",
        "        '\\n',\n",
        "        '[',\n",
        "    ],\n",
        "    verbose = True,\n",
        "):\n",
        "\n",
        "    \"\"\" this function clips the generated text\n",
        "    and extracts out the text between a\n",
        "    pre-specified prefix and suffix\n",
        "\n",
        "    the prefix could be the enture input text\n",
        "    the suffix is often the delimiter such as \n",
        "    the next line \\n token or a period . .\n",
        "    \"\"\"\n",
        "\n",
        "    if prefix is not None:\n",
        "        reply = reply[len(prefix):]\n",
        "\n",
        "    if verbose:\n",
        "        print('predicted future:')\n",
        "        print(repr(reply))\n",
        "    \n",
        "    for string in stop_strings:\n",
        "        if string in reply:\n",
        "            reply = reply[:reply.index(string)]\n",
        "    \n",
        "    return reply.strip()\n",
        "\n",
        "\n",
        "def convo_list_dic2list_str(\n",
        "  conversation_list_dic,\n",
        "  human_symbol = '[H]: ',\n",
        "  bot_symbol = '[B]: ',\n",
        "  utterance_delimiter = '\\n',\n",
        "):\n",
        "\n",
        "  \"\"\" This function takes a list of dictionaries\n",
        "  and turns them into a list of speaker_symbol + utterance strings\n",
        "\n",
        "  Args: \n",
        "      conversation_list_dic (List[Dict]): \n",
        "      ie: [{'speaker': 'bot', 'utterance': 'im waking up!'},\n",
        "           {'speaker': 'human', 'utterance': 'wakey wakey sleepyhead'}, ...]\n",
        "\n",
        "  Returns:\n",
        "      conversation_list_str (List[str]): list of speaker_symbol + utterance strings\n",
        "      ie: ['\\n[C]: Hello Fara.','\\n[A]: Hello! How are you doing today?',...]\n",
        "  \"\"\"\n",
        "\n",
        "  speaker2symbol = {\n",
        "      'bot':bot_symbol,\n",
        "      'human':human_symbol,\n",
        "  }\n",
        "\n",
        "  conversation_list_str = list()\n",
        "\n",
        "  for u in conversation_list_dic:\n",
        "\n",
        "      speaker_symbol = speaker2symbol[u['speaker']]\n",
        "      utterance = end_punctuation(u['utterance'])\n",
        "\n",
        "      conversation_list_str.append(utterance_delimiter + speaker_symbol + utterance)\n",
        "\n",
        "  # Elicit next agent utterance\n",
        "  conversation_list_str.append(utterance_delimiter + bot_symbol)\n",
        "\n",
        "  return conversation_list_str\n",
        "\n",
        "def generate_extract_replies(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    max_gen_len = 16, \n",
        "    no_repeat_ngram_size = None,\n",
        "    pad_token_id = 50256,\n",
        "    do_sample = True,\n",
        "    top_k = 100, \n",
        "    top_p = 0.99, \n",
        "    num_return_sequences = 1,\n",
        "    temperature = 0.9,\n",
        "    stop_strings = [\n",
        "        '<',\n",
        "        '[human]',\n",
        "        '\\n',\n",
        "        '[',\n",
        "    ],\n",
        "    verbose = False,\n",
        "):\n",
        "\n",
        "    ''' This function predicts the next utterance\n",
        "    in a conversation\n",
        "    '''\n",
        "\n",
        "    gen_texts = generate_text(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        prompt,\n",
        "        max_gen_len = max_gen_len, \n",
        "        no_repeat_ngram_size = no_repeat_ngram_size,\n",
        "        pad_token_id = pad_token_id,\n",
        "        do_sample = do_sample,\n",
        "        top_k = top_k, \n",
        "        top_p = top_p, \n",
        "        num_return_sequences = num_return_sequences,\n",
        "        temperature = temperature,\n",
        "        verbose = verbose,\n",
        "    )\n",
        "\n",
        "    replies = [\n",
        "        extract_str(\n",
        "            gen_text,\n",
        "            prefix = prompt,\n",
        "            stop_strings = stop_strings,\n",
        "            verbose = verbose,\n",
        "        )\n",
        "        for gen_text in gen_texts\n",
        "    ]\n",
        "\n",
        "    return replies\n",
        "\n",
        "\n",
        "def generate_text(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    max_gen_len = 16, \n",
        "    no_repeat_ngram_size = None,\n",
        "    pad_token_id = 50256,\n",
        "    do_sample = True,\n",
        "    top_k = 100, \n",
        "    top_p = 0.99, \n",
        "    num_return_sequences = 1,\n",
        "    temperature = 0.9,\n",
        "    verbose = False,\n",
        "):\n",
        "\n",
        "    ''' function for generating text from an input into \n",
        "    the app.package model\n",
        "\n",
        "    prompt (str): text to be tokenized and pushed through model\n",
        "\n",
        "    if you are doing few shot detection you should leave \n",
        "    no_repeat_ngram_size = None and max_len = 16\n",
        "    as long as the default max_len is more than the expected\n",
        "    label text\n",
        "\n",
        "    we leave it up to the label extractor to clip of the portion\n",
        "    of the generated text that you need\n",
        "    '''\n",
        "    NUM_GPUS = torch.cuda.device_count()\n",
        "\n",
        "    prompt_dic = tokenizer(prompt,return_tensors=\"pt\")\n",
        "    prompt_ids = prompt_dic.input_ids\n",
        "    prompt_mask = prompt_dic.attention_mask\n",
        "    prompt_len = prompt_ids.shape[1]\n",
        "\n",
        "    if verbose:\n",
        "        print('prompt_ids.shape', prompt_ids.shape)\n",
        "        print('prompt_mask.shape', prompt_mask.shape)\n",
        "\n",
        "    if NUM_GPUS > 0:\n",
        "        prompt_ids = prompt_ids.to(model.device)\n",
        "        prompt_mask = prompt_mask.to(model.device)\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        prompt_ids,\n",
        "        attention_mask = prompt_mask,\n",
        "        max_length = prompt_len + max_gen_len,\n",
        "        no_repeat_ngram_size = no_repeat_ngram_size,\n",
        "        pad_token_id = pad_token_id,\n",
        "        do_sample = do_sample,\n",
        "        top_k = top_k, \n",
        "        top_p = top_p, \n",
        "        num_return_sequences = num_return_sequences,\n",
        "        temperature = temperature,\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.batch_decode(output_ids)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self, model, tokenizer):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dialog_history = example_dialog_history\n",
        "\n",
        "    def initiate_conversation(self,):\n",
        "\n",
        "        initial_utterance = \"Hello! who are you?\"\n",
        "\n",
        "        self.dialog_history = [{'speaker':'bot','utterance':initial_utterance}]\n",
        "\n",
        "        return initial_utterance\n",
        "\n",
        "    def receive_respond(self, \n",
        "        input_utterance, \n",
        "        symbol_utter_separator=': ',\n",
        "        utterance_delimiter = '\\n',\n",
        "        human_symbol = '[Human]',\n",
        "        bot_symbol = '[AI]',\n",
        "        verbose = False,\n",
        "    ):\n",
        "\n",
        "        input_utterance = input_utterance.strip()\n",
        "\n",
        "        background_prompt =  get_background_prompt(\n",
        "            human_symbol = human_symbol,\n",
        "            bot_symbol = bot_symbol,\n",
        "        )\n",
        "\n",
        "        self.dialog_history.append({'speaker':'human','utterance':input_utterance})\n",
        "\n",
        "        convo_list_str = convo_list_dic2list_str(\n",
        "            self.dialog_history,\n",
        "            human_symbol = human_symbol+symbol_utter_separator,\n",
        "            bot_symbol = bot_symbol+symbol_utter_separator,\n",
        "            utterance_delimiter = utterance_delimiter,\n",
        "        )\n",
        "\n",
        "        background_dialog_prompt = (background_prompt + ''.join(convo_list_str)).strip()\n",
        "\n",
        "        if verbose:\n",
        "            print(repr(background_dialog_prompt))\n",
        "\n",
        "        replies = generate_extract_replies(\n",
        "            model = self.model,\n",
        "            tokenizer = self.tokenizer,\n",
        "            prompt = background_dialog_prompt,\n",
        "            max_gen_len = 32, \n",
        "            no_repeat_ngram_size = 3,\n",
        "            pad_token_id = self.tokenizer.eos_token_id,\n",
        "            do_sample = True,\n",
        "            top_k = 80, \n",
        "            top_p = 0.8, \n",
        "            num_return_sequences = 1,\n",
        "            temperature = 0.8,\n",
        "            stop_strings = [\n",
        "                human_symbol,\n",
        "                '\\n',\n",
        "            ],\n",
        "            verbose = False, \n",
        "        )\n",
        "\n",
        "        if verbose:\n",
        "            print(replies)\n",
        "\n",
        "        self.dialog_history.append({'speaker':'bot','utterance':replies[0]})\n",
        "\n",
        "        return replies[0]\n",
        "\n",
        "\n",
        "def chat(agent, verbose):\n",
        "\n",
        "    print(agent.initiate_conversation())\n",
        "\n",
        "    while True:\n",
        "\n",
        "        statement = input(\"you> \")\n",
        "        print(\"agent> \",agent.receive_respond(statement, verbose = verbose))\n",
        "\n",
        "        if statement == \"quit\":\n",
        "            break"
      ],
      "metadata": {
        "id": "y_8ShOXIFFF_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Model and the tokenizer\n",
        "\n",
        "the function below will print out the number of parameters in billions and confirm if the model was successfully placed on the GPU or left on CPU"
      ],
      "metadata": {
        "id": "Z1gOfZaIOI0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_load_path = 'EleutherAI/gpt-j-6B' #'gpt2-large' #'gpt2-xl'\n",
        "\n",
        "model, tokenizer = get_model_tokenizer(\n",
        "      model_load_path = model_load_path,\n",
        "      tokenizer_name = model_load_path,\n",
        "      cache_dir = '../modelstates/'+model_load_path,\n",
        "      model_device = None,\n",
        "      verbose=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "4bfcb45e8ad04146926095cad723d52f",
            "6a9737e250104f73bf5f34801d277baf",
            "d32688dbf90b49b39799bfd673d8a155",
            "83349b366ce8495a81834ec875d87e20",
            "91f58d08ec8f48908fa3cc8dcc41dd59",
            "ea3698016e1847daa7328b57581eef5f",
            "70c78e29fb0a43559f43c25b40ae8d13",
            "fcf14912c80a4a1684312f961bb3c9fc",
            "f762a4def5dd4d78b0abbe3670e7180a",
            "81f908249fc744bf8bb64be5c8033bc8",
            "d4806a5e373d445bb759abba2903a2a6",
            "6f3b38477e904eae9031d5cbe6b139bf",
            "7818b2ced5ce43f98548115e3218e27e",
            "62014d34810943cfb1b02dbde471efed",
            "d06e287c559d4b9fb28a36d58cd49caf",
            "3fd7d34e64204dd5a607264baffb7fac",
            "fd873a1b96cb4975b54186f818150420",
            "bfe89ed19fa0403d9a11d87c24fb8bd0",
            "21aff844bf7d4db69b036742dba97226",
            "436fd5a7e72349d6a47e63d2223e22e9",
            "4f9c863520fd4b418377f07dae6f41c0",
            "5f4b646c6da24eee9a356196403e74ee"
          ]
        },
        "collapsed": true,
        "id": "H8E-2BycLZ5X",
        "outputId": "c3de28f3-7791-4a87-dadd-8576757dc040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/836 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4bfcb45e8ad04146926095cad723d52f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/12.1G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f3b38477e904eae9031d5cbe6b139bf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiate an agent and talk to it\n",
        "\n",
        "set `verbose = True` to see the whole input prompt to the "
      ],
      "metadata": {
        "id": "jvHIXzSrOdo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(model, tokenizer)\n",
        "\n",
        "chat(agent, verbose = False)"
      ],
      "metadata": {
        "id": "AiBzFemrMYaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QkZZcw8ROsni"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}