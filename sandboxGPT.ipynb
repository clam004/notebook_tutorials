{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOy8SMIASajRP5ZcxZVLrY8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b539949058a945068c1e71e0d93cebe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b928d474bc240f49c32fc15d806d0ae",
              "IPY_MODEL_14e768c90c4541a19b69db620c8a4b95",
              "IPY_MODEL_742dbcd3306941b7a27e5be3da9b2bce"
            ],
            "layout": "IPY_MODEL_92676820e6b345efa7318dd38fb7b8f9"
          }
        },
        "3b928d474bc240f49c32fc15d806d0ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c839db792872438f8246e9886dc9fcb4",
            "placeholder": "​",
            "style": "IPY_MODEL_5b7c95d137e94ef395289bcae19504a7",
            "value": ""
          }
        },
        "14e768c90c4541a19b69db620c8a4b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92ad86c65ea84cfb834ffc784d5796e7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d9cab9eeb4b444291d24bca2cb0da0a",
            "value": 0
          }
        },
        "742dbcd3306941b7a27e5be3da9b2bce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_992c668791bf40a4847bab87d06c859a",
            "placeholder": "​",
            "style": "IPY_MODEL_20483380cf2c499b95783766bffb389d",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "92676820e6b345efa7318dd38fb7b8f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c839db792872438f8246e9886dc9fcb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b7c95d137e94ef395289bcae19504a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92ad86c65ea84cfb834ffc784d5796e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8d9cab9eeb4b444291d24bca2cb0da0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "992c668791bf40a4847bab87d06c859a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20483380cf2c499b95783766bffb389d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clam004/notebook_tutorials/blob/main/sandboxGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minimal Causal Large Language Model (LLM) Chatbot\n",
        "\n",
        "run the next two cells to install and import PyTorch and huggingface "
      ],
      "metadata": {
        "id": "JGkuWFiyNpi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install transformers accelerate"
      ],
      "metadata": {
        "id": "_BrcYn9CFWTY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "b539949058a945068c1e71e0d93cebe1",
            "3b928d474bc240f49c32fc15d806d0ae",
            "14e768c90c4541a19b69db620c8a4b95",
            "742dbcd3306941b7a27e5be3da9b2bce",
            "92676820e6b345efa7318dd38fb7b8f9",
            "c839db792872438f8246e9886dc9fcb4",
            "5b7c95d137e94ef395289bcae19504a7",
            "92ad86c65ea84cfb834ffc784d5796e7",
            "8d9cab9eeb4b444291d24bca2cb0da0a",
            "992c668791bf40a4847bab87d06c859a",
            "20483380cf2c499b95783766bffb389d"
          ]
        },
        "id": "M5bcvqPAEDUW",
        "outputId": "fc6d7492-1de1-46da-b305-cebaffb9a572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.__version__ 1.12.1+cu113\n",
            "torch.cuda.device_count() 1\n",
            "torch.cuda.empty_cache() None\n",
            "Moving 0 files to the new cache system\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b539949058a945068c1e71e0d93cebe1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.22.1\n"
          ]
        }
      ],
      "source": [
        "#sys libs\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import datetime\n",
        "from datetime import date\n",
        "import calendar\n",
        "import pytz\n",
        "\n",
        "#data manupulation libs\n",
        "import numpy as np\n",
        "\n",
        "#string manupulation libs\n",
        "import re\n",
        "import string\n",
        "\n",
        "#torch libs\n",
        "import torch\n",
        "print('torch.__version__', torch.__version__)\n",
        "print('torch.cuda.device_count()', torch.cuda.device_count())\n",
        "print('torch.cuda.empty_cache()', torch.cuda.empty_cache())\n",
        "\n",
        "#huggingface transformers\n",
        "import transformers\n",
        "print(transformers.__version__)\n",
        "from transformers import set_seed\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import GPTJForCausalLM\n",
        "\n",
        "# seeds\n",
        "set_seed(42)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU acceleration\n",
        "\n",
        "To give yourself a GPU in colab, go to `Runtime`-->`Change runtime type`\n",
        "\n",
        "You can confirm this worked because if you run the above cell again `torch.cuda.device_count()` will change from 0 to the number of GPUs PyTorch now recognizes, this would be 0 to 1 in colab. \n",
        "\n",
        "\n",
        "### The Agent and Environment\n",
        "\n",
        "For transparency the cell below has all the functions and classes that run this minimal demo.\n",
        "\n",
        "It is very long, but dont worry you just have to run it and scroll all the way to the bottom to get to the demo. \n",
        "\n",
        "You can make code changes and quickly see the results, for example, change the get_background_prompt() function to chnage the initial part of the LLM input prompt that come before the dialog history portion of the final prompt. "
      ],
      "metadata": {
        "id": "9Z8UNY7vEiC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_dialog_history = [\n",
        "    {'speaker':'bot','utterance':'Hello! who are you?'},\n",
        "    {'speaker':'human','utterance':'my name is baby force'},\n",
        "    {'speaker':'bot','utterance':'hi baby, or is it Mr. Force?'},\n",
        "    {'speaker':'human','utterance':'actually im a baby girl and my first name is Force'},\n",
        "    {'speaker':'bot','utterance':'thats a weird name for a baby girl isnt it?'},\n",
        "    {'speaker':'human','utterance':'its a weird name for any human'},\n",
        "]\n",
        "\n",
        "\n",
        "def get_background_prompt(\n",
        "    human_symbol = '[H]',\n",
        "    bot_symbol = '[B]',\n",
        "):\n",
        "\n",
        "    \"\"\" a background_prompt describing what the conversation is\n",
        "    (e.g. This is a conversation between [{client_name}], a person, and [{robot_name}] ...)\n",
        "    \"\"\"\n",
        "\n",
        "    utc_now = pytz.utc.localize(datetime.datetime.utcnow())\n",
        "    pst_now = utc_now.astimezone(pytz.timezone(\"America/Los_Angeles\"))\n",
        "    curr_date = date.today()\n",
        "    day_of_week = calendar.day_name[curr_date.weekday()]\n",
        "\n",
        "    local_date = pst_now.strftime(\"%m/%d/%Y\")\n",
        "    local_time = pst_now.strftime(\"%I:%M %p\")\n",
        "\n",
        "    background_prompt = \\\n",
        "    \"This is a text messaging only conversation between \"+human_symbol+\" and \"+bot_symbol+\". \"+\\\n",
        "    bot_symbol+\" is an artificial intelligence or AI. \"+\\\n",
        "    bot_symbol+\" knows it is just a computer program. \"+\\\n",
        "    bot_symbol+\" speaks in a manner that is kind, empathetic and \"+\\\n",
        "    \"is programmed to keep \"+human_symbol+\" safe. \"+\\\n",
        "    bot_symbol+\" likes to tell helpful stories. \"+\\\n",
        "    \"Today's date is \"+day_of_week+\" \"+local_date+\", the time is \"+local_time+\". \"+\\\n",
        "    \"\\n \"\n",
        "\n",
        "    return background_prompt\n",
        "\n",
        "\n",
        "def get_model_tokenizer(\n",
        "    model_load_path = 'EleutherAI/gpt-j-6B',\n",
        "    tokenizer_name = 'EleutherAI/gpt-j-6B',\n",
        "    cache_dir = None,\n",
        "    model_device = None,\n",
        "    verbose = False,\n",
        "):\n",
        "\n",
        "    ''' This is a function to clean up the model preparations, GPU/CPU loading \n",
        "    and matching tokenizer\n",
        "\n",
        "    model architecture is based on the tokenizer name\n",
        "\n",
        "    set model_device = 'cpu' to force model onto CPU despite having GPUs \n",
        "    available, or to torch.device('cuda:3') to get it to the fourth GPU, etc.\n",
        "    if no GPUs available, cpu is the default. \n",
        "    '''\n",
        "\n",
        "    NUM_GPUS = torch.cuda.device_count()\n",
        "\n",
        "    if tokenizer_name in ['distilgpt2', 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']:\n",
        "\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(\n",
        "            tokenizer_name,\n",
        "            pad_token='<|endoftext|>',\n",
        "            padding_side = 'left',\n",
        "        )\n",
        "\n",
        "        model = GPT2LMHeadModel.from_pretrained(\n",
        "            model_load_path,\n",
        "            cache_dir = cache_dir, \n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    elif tokenizer_name in ['EleutherAI/gpt-j-6B']:\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            tokenizer_name,\n",
        "            pad_token='<|endoftext|>',\n",
        "            padding_side = 'left',\n",
        "        )\n",
        "\n",
        "        if NUM_GPUS > 0:\n",
        "          \n",
        "          model = GPTJForCausalLM.from_pretrained(\n",
        "              model_load_path,\n",
        "              revision='float16', \n",
        "              torch_dtype=torch.float16, \n",
        "              low_cpu_mem_usage=True,\n",
        "              cache_dir = cache_dir, \n",
        "          )\n",
        "\n",
        "        else:\n",
        "\n",
        "          model = GPTJForCausalLM.from_pretrained(\n",
        "              model_load_path,\n",
        "              cache_dir = cache_dir, \n",
        "          )\n",
        "\n",
        "    else:\n",
        "\n",
        "        if verbose:\n",
        "            print('no match for tokenizer found')\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    if model_device is not None:\n",
        "        model = model.to(model_device)\n",
        "    elif NUM_GPUS == 1:\n",
        "        if verbose:\n",
        "            print('model = model.cuda()')\n",
        "        model = model.cuda()\n",
        "    elif NUM_GPUS > 1 and tokenizer_name in ['distilgpt2', 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'EleutherAI/gpt-j-6B']:\n",
        "        # break up model and place model components on different GPUs\n",
        "        if verbose:\n",
        "            print('model.parallelize()')\n",
        "        model.parallelize()\n",
        "    else:\n",
        "        if verbose:\n",
        "            print('did not place model on any GPUs, model_device = \\'cpu\\'')\n",
        "\n",
        "    if verbose:\n",
        "        print('model.device', model.device)\n",
        "        print(\"num_params\", \n",
        "            sum(p.numel() for p in model.parameters() if p.requires_grad)/1e9,\n",
        "            \"B\"\n",
        "        ) \n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def end_punctuation(utter):\n",
        "    \n",
        "    if len(utter) > 0:\n",
        "      if utter[-1] not in [\"?\",\"!\",\".\"]:\n",
        "          utter+=\".\"\n",
        "        \n",
        "    return utter\n",
        "\n",
        "\n",
        "def extract_str(\n",
        "    reply, \n",
        "    prefix = None,\n",
        "    stop_strings = [\n",
        "        '<',\n",
        "        '[human]',\n",
        "        '\\n',\n",
        "        '[',\n",
        "    ],\n",
        "    verbose = True,\n",
        "):\n",
        "\n",
        "    \"\"\" this function clips the generated text\n",
        "    and extracts out the text between a\n",
        "    pre-specified prefix and suffix\n",
        "\n",
        "    the prefix could be the enture input text\n",
        "    the suffix is often the delimiter such as \n",
        "    the next line \\n token or a period . .\n",
        "    \"\"\"\n",
        "\n",
        "    if prefix is not None:\n",
        "        reply = reply[len(prefix):]\n",
        "\n",
        "    if verbose:\n",
        "        print('predicted future:')\n",
        "        print(repr(reply))\n",
        "    \n",
        "    for string in stop_strings:\n",
        "        if string in reply:\n",
        "            reply = reply[:reply.index(string)]\n",
        "    \n",
        "    return reply.strip()\n",
        "\n",
        "\n",
        "def convo_list_dic2list_str(\n",
        "  conversation_list_dic,\n",
        "  human_symbol = '[H]: ',\n",
        "  bot_symbol = '[B]: ',\n",
        "  utterance_delimiter = '\\n',\n",
        "):\n",
        "\n",
        "  \"\"\" This function takes a list of dictionaries\n",
        "  and turns them into a list of speaker_symbol + utterance strings\n",
        "\n",
        "  Args: \n",
        "      conversation_list_dic (List[Dict]): \n",
        "      ie: [{'speaker': 'bot', 'utterance': 'im waking up!'},\n",
        "           {'speaker': 'human', 'utterance': 'wakey wakey sleepyhead'}, ...]\n",
        "\n",
        "  Returns:\n",
        "      conversation_list_str (List[str]): list of speaker_symbol + utterance strings\n",
        "      ie: ['\\n[C]: Hello Fara.','\\n[A]: Hello! How are you doing today?',...]\n",
        "  \"\"\"\n",
        "\n",
        "  speaker2symbol = {\n",
        "      'bot':bot_symbol,\n",
        "      'human':human_symbol,\n",
        "  }\n",
        "\n",
        "  conversation_list_str = list()\n",
        "\n",
        "  for u in conversation_list_dic:\n",
        "\n",
        "      speaker_symbol = speaker2symbol[u['speaker']]\n",
        "      utterance = end_punctuation(u['utterance'])\n",
        "\n",
        "      conversation_list_str.append(utterance_delimiter + speaker_symbol + utterance)\n",
        "\n",
        "  # Elicit next agent utterance\n",
        "  conversation_list_str.append(utterance_delimiter + bot_symbol)\n",
        "\n",
        "  return conversation_list_str\n",
        "\n",
        "def generate_extract_replies(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    max_gen_len = 16, \n",
        "    no_repeat_ngram_size = None,\n",
        "    pad_token_id = 50256,\n",
        "    do_sample = True,\n",
        "    top_k = 100, \n",
        "    top_p = 0.99, \n",
        "    num_return_sequences = 1,\n",
        "    temperature = 0.9,\n",
        "    stop_strings = [\n",
        "        '<',\n",
        "        '[human]',\n",
        "        '\\n',\n",
        "        '[',\n",
        "    ],\n",
        "    verbose = False,\n",
        "):\n",
        "\n",
        "    ''' This function predicts the next utterance\n",
        "    in a conversation\n",
        "    '''\n",
        "\n",
        "    gen_texts = generate_text(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        prompt,\n",
        "        max_gen_len = max_gen_len, \n",
        "        no_repeat_ngram_size = no_repeat_ngram_size,\n",
        "        pad_token_id = pad_token_id,\n",
        "        do_sample = do_sample,\n",
        "        top_k = top_k, \n",
        "        top_p = top_p, \n",
        "        num_return_sequences = num_return_sequences,\n",
        "        temperature = temperature,\n",
        "        verbose = verbose,\n",
        "    )\n",
        "\n",
        "    replies = [\n",
        "        extract_str(\n",
        "            gen_text,\n",
        "            prefix = prompt,\n",
        "            stop_strings = stop_strings,\n",
        "            verbose = verbose,\n",
        "        )\n",
        "        for gen_text in gen_texts\n",
        "    ]\n",
        "\n",
        "    return replies\n",
        "\n",
        "\n",
        "def generate_text(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    max_gen_len = 16, \n",
        "    no_repeat_ngram_size = None,\n",
        "    pad_token_id = 50256,\n",
        "    do_sample = True,\n",
        "    top_k = 100, \n",
        "    top_p = 0.99, \n",
        "    num_return_sequences = 1,\n",
        "    temperature = 0.9,\n",
        "    verbose = False,\n",
        "):\n",
        "\n",
        "    ''' function for generating text from an input into \n",
        "    the app.package model\n",
        "\n",
        "    prompt (str): text to be tokenized and pushed through model\n",
        "\n",
        "    if you are doing few shot detection you should leave \n",
        "    no_repeat_ngram_size = None and max_len = 16\n",
        "    as long as the default max_len is more than the expected\n",
        "    label text\n",
        "\n",
        "    we leave it up to the label extractor to clip of the portion\n",
        "    of the generated text that you need\n",
        "    '''\n",
        "    NUM_GPUS = torch.cuda.device_count()\n",
        "\n",
        "    prompt_dic = tokenizer(prompt,return_tensors=\"pt\")\n",
        "    prompt_ids = prompt_dic.input_ids\n",
        "    prompt_mask = prompt_dic.attention_mask\n",
        "    prompt_len = prompt_ids.shape[1]\n",
        "\n",
        "    if verbose:\n",
        "        print('prompt_ids.shape', prompt_ids.shape)\n",
        "        print('prompt_mask.shape', prompt_mask.shape)\n",
        "\n",
        "    if NUM_GPUS > 0:\n",
        "        prompt_ids = prompt_ids.to(model.device)\n",
        "        prompt_mask = prompt_mask.to(model.device)\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        prompt_ids,\n",
        "        attention_mask = prompt_mask,\n",
        "        max_length = prompt_len + max_gen_len,\n",
        "        no_repeat_ngram_size = no_repeat_ngram_size,\n",
        "        pad_token_id = pad_token_id,\n",
        "        do_sample = do_sample,\n",
        "        top_k = top_k, \n",
        "        top_p = top_p, \n",
        "        num_return_sequences = num_return_sequences,\n",
        "        temperature = temperature,\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.batch_decode(output_ids)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self, model, tokenizer):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dialog_history = example_dialog_history\n",
        "\n",
        "    def initiate_conversation(self,):\n",
        "\n",
        "        initial_utterance = \"Hello! who are you?\"\n",
        "\n",
        "        self.dialog_history = [{'speaker':'bot','utterance':initial_utterance}]\n",
        "\n",
        "        return initial_utterance\n",
        "\n",
        "    def receive_respond(self, \n",
        "        input_utterance, \n",
        "        symbol_utter_separator=': ',\n",
        "        utterance_delimiter = '\\n',\n",
        "        human_symbol = '[Human]',\n",
        "        bot_symbol = '[AI]',\n",
        "        verbose = False,\n",
        "    ):\n",
        "\n",
        "        input_utterance = input_utterance.strip()\n",
        "\n",
        "        background_prompt =  get_background_prompt(\n",
        "            human_symbol = human_symbol,\n",
        "            bot_symbol = bot_symbol,\n",
        "        )\n",
        "\n",
        "        self.dialog_history.append({'speaker':'human','utterance':input_utterance})\n",
        "\n",
        "        convo_list_str = convo_list_dic2list_str(\n",
        "            self.dialog_history,\n",
        "            human_symbol = human_symbol+symbol_utter_separator,\n",
        "            bot_symbol = bot_symbol+symbol_utter_separator,\n",
        "            utterance_delimiter = utterance_delimiter,\n",
        "        )\n",
        "\n",
        "        background_dialog_prompt = (background_prompt + ''.join(convo_list_str)).strip()\n",
        "\n",
        "        if verbose:\n",
        "            print(repr(background_dialog_prompt))\n",
        "\n",
        "        replies = generate_extract_replies(\n",
        "            model = self.model,\n",
        "            tokenizer = self.tokenizer,\n",
        "            prompt = background_dialog_prompt,\n",
        "            max_gen_len = 32, \n",
        "            no_repeat_ngram_size = 3,\n",
        "            pad_token_id = self.tokenizer.eos_token_id,\n",
        "            do_sample = True,\n",
        "            top_k = 80, \n",
        "            top_p = 0.8, \n",
        "            num_return_sequences = 1,\n",
        "            temperature = 0.8,\n",
        "            stop_strings = [\n",
        "                human_symbol,\n",
        "                '\\n',\n",
        "            ],\n",
        "            verbose = False, \n",
        "        )\n",
        "\n",
        "        if verbose:\n",
        "            print(replies)\n",
        "\n",
        "        self.dialog_history.append({'speaker':'bot','utterance':replies[0]})\n",
        "\n",
        "        return replies[0]\n",
        "\n",
        "\n",
        "def chat(agent, pre_loaded_history = None, verbose = False):\n",
        "\n",
        "    if pre_loaded_history is not None:\n",
        "      agent.dialog_history = pre_loaded_history \n",
        "      print(agent.dialog_history) \n",
        "    else:\n",
        "      print(agent.initiate_conversation())\n",
        "\n",
        "    while True:\n",
        "\n",
        "        statement = input(\"you> \")\n",
        "        print(\"agent>\",agent.receive_respond(statement, verbose = verbose))\n",
        "\n",
        "        if statement == \"quit\":\n",
        "            break"
      ],
      "metadata": {
        "id": "y_8ShOXIFFF_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Model and the tokenizer\n",
        "\n",
        "the function below will print out the number of parameters in billions and confirm if the model was successfully placed on the GPU or left on CPU"
      ],
      "metadata": {
        "id": "Z1gOfZaIOI0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_load_path = 'gpt2-large' # 'gpt2' # 'gpt2-medium' # 'gpt2-xl' #'EleutherAI/gpt-j-6B' #\n",
        "\n",
        "model, tokenizer = get_model_tokenizer(\n",
        "      model_load_path = model_load_path,\n",
        "      tokenizer_name = model_load_path,\n",
        "      cache_dir = '../modelstates/'+model_load_path,\n",
        "      model_device = None,\n",
        "      verbose=True,\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "H8E-2BycLZ5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Talk to the LLM\n",
        "\n",
        "set `verbose = True` to see the whole input prompt to the "
      ],
      "metadata": {
        "id": "jvHIXzSrOdo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(model, tokenizer)\n",
        "\n",
        "chat(agent, verbose = False)"
      ],
      "metadata": {
        "id": "AiBzFemrMYaF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QkZZcw8ROsni"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}